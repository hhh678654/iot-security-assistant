# ollama_client.py - æœ¬åœ°å¤§æ¨¡å‹
import json
import logging
import time
import hashlib
import pickle
import os
import subprocess
import platform
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from datetime import datetime, timedelta


@dataclass
class OllamaResponse:
    """Ollamaå“åº”æ•°æ®ç»“æ„"""
    content: str
    model: str
    tokens_used: int
    response_time: float
    metadata: Dict = None

    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}


class OllamaCache:
    """æœ¬åœ°ç¼“å­˜ç³»ç»Ÿ"""

    def __init__(self, cache_dir: str = "cache", max_age_hours: int = 24):
        self.cache_dir = cache_dir
        self.max_age = timedelta(hours=max_age_hours)
        os.makedirs(cache_dir, exist_ok=True)

    def _get_cache_key(self, prompt: str, model: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        content = f"{prompt}:{model}"
        return hashlib.md5(content.encode()).hexdigest()

    def get(self, prompt: str, model: str) -> Optional[OllamaResponse]:
        """è·å–ç¼“å­˜å“åº”"""
        cache_key = self._get_cache_key(prompt, model)
        cache_file = os.path.join(self.cache_dir, f"{cache_key}.pkl")

        if os.path.exists(cache_file):
            try:
                with open(cache_file, 'rb') as f:
                    cached_data = pickle.load(f)

                # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ
                cache_time = datetime.fromisoformat(cached_data['timestamp'])
                if datetime.now() - cache_time < self.max_age:
                    return cached_data['response']
                else:
                    # ç¼“å­˜è¿‡æœŸï¼Œåˆ é™¤æ–‡ä»¶
                    os.remove(cache_file)
            except Exception as e:
                # ç¼“å­˜æ–‡ä»¶æŸåï¼Œåˆ é™¤
                try:
                    os.remove(cache_file)
                except:
                    pass

        return None

    def set(self, prompt: str, model: str, response: OllamaResponse):
        """è®¾ç½®ç¼“å­˜"""
        cache_key = self._get_cache_key(prompt, model)
        cache_file = os.path.join(self.cache_dir, f"{cache_key}.pkl")

        cached_data = {
            'timestamp': datetime.now().isoformat(),
            'response': response
        }

        try:
            with open(cache_file, 'wb') as f:
                pickle.dump(cached_data, f)
        except Exception as e:
            logging.error(f"ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")


class OllamaClient:
    """ä¼˜åŒ–çš„Ollamaå®¢æˆ·ç«¯"""

    def __init__(self, model_name: str = "llama3",  # é»˜è®¤ä½¿ç”¨æ‚¨çš„llama3
                 base_url: str = "http://localhost:11434",
                 timeout: int = 60):
        self.model_name = model_name
        self.base_url = base_url
        self.timeout = timeout
        self.cache = OllamaCache()
        self.logger = logging.getLogger(__name__)

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "requests": 0,
            "cache_hits": 0,
            "total_tokens": 0,
            "total_time": 0.0,
            "successful_requests": 0,
            "failed_requests": 0
        }

    def is_ollama_installed(self) -> bool:
        """æ£€æŸ¥Ollamaæ˜¯å¦å®‰è£…ï¼ˆæ”¯æŒWindowsï¼‰"""
        try:
            # Windowsä¸‹å¯èƒ½éœ€è¦ä¸åŒçš„æ£€æŸ¥æ–¹å¼
            ollama_commands = ["ollama", "ollama.exe"]

            for cmd in ollama_commands:
                try:
                    result = subprocess.run(
                        [cmd, "--version"],
                        capture_output=True,
                        text=True,
                        timeout=5,
                        shell=(platform.system() == "Windows")
                    )
                    if result.returncode == 0:
                        return True
                except (FileNotFoundError, subprocess.TimeoutExpired):
                    continue

            return False

        except Exception as e:
            self.logger.debug(f"æ£€æŸ¥Ollamaå®‰è£…å¤±è´¥: {e}")
            return False

    def is_available(self) -> bool:
        """æ£€æŸ¥OllamaæœåŠ¡æ˜¯å¦å¯ç”¨"""
        try:
            import requests
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            return response.status_code == 200
        except Exception as e:
            self.logger.debug(f"OllamaæœåŠ¡ä¸å¯ç”¨: {e}")
            return False

    def list_models(self) -> List[str]:
        """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        try:
            import requests
            response = requests.get(f"{self.base_url}/api/tags", timeout=10)
            if response.status_code == 200:
                data = response.json()
                models = []
                for model in data.get("models", []):
                    model_name = model.get("name", "")
                    # å¤„ç†æ¨¡å‹åç§°æ ¼å¼åŒ–ï¼ˆå¦‚ llama3:latest -> llama3ï¼‰
                    if ":" in model_name:
                        base_name = model_name.split(":")[0]
                        models.extend([model_name, base_name])
                    else:
                        models.append(model_name)
                return list(set(models))  # å»é‡
            return []
        except Exception as e:
            self.logger.error(f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
            return []

    def pull_model(self, model_name: str = None) -> bool:
        """æ‹‰å–æ¨¡å‹"""
        model = model_name or self.model_name
        try:
            import requests

            print(f"ğŸ”„ æ­£åœ¨æ‹‰å–æ¨¡å‹: {model}")
            response = requests.post(
                f"{self.base_url}/api/pull",
                json={"name": model},
                stream=True,
                timeout=600  # 10åˆ†é’Ÿè¶…æ—¶
            )

            if response.status_code == 200:
                for line in response.iter_lines():
                    if line:
                        try:
                            data = json.loads(line)
                            if "status" in data:
                                status = data["status"]
                                if "completed" in data and "total" in data:
                                    progress = data["completed"] / data["total"] * 100
                                    print(f"ğŸ“¥ {status}: {progress:.1f}%")
                                else:
                                    print(f"ğŸ“¥ {status}")
                        except json.JSONDecodeError:
                            pass
                print(f"âœ… æ¨¡å‹æ‹‰å–å®Œæˆ: {model}")
                return True
            else:
                print(f"âŒ æ¨¡å‹æ‹‰å–å¤±è´¥: {response.text}")
                return False

        except Exception as e:
            print(f"âŒ æ¨¡å‹æ‹‰å–é”™è¯¯: {e}")
            return False

    def generate(self, prompt: str, max_tokens: int = 2000,
                 temperature: float = 0.7, stream: bool = False) -> OllamaResponse:
        """ç”Ÿæˆå“åº”"""
        self.stats["requests"] += 1

        # 1. æ£€æŸ¥ç¼“å­˜
        cached_response = self.cache.get(prompt, self.model_name)
        if cached_response:
            self.stats["cache_hits"] += 1
            self.logger.info("ä½¿ç”¨ç¼“å­˜å“åº”")
            return cached_response

        # 2. è°ƒç”¨Ollama API
        try:
            import requests

            start_time = time.time()

            # å‡†å¤‡è¯·æ±‚æ•°æ®
            data = {
                "model": self.model_name,
                "prompt": prompt,
                "stream": stream,
                "options": {
                    "temperature": temperature,
                    "num_predict": max_tokens,
                    "stop": ["Human:", "Assistant:", "User:", "AI:", "\n\nHuman:", "\n\nUser:"]
                }
            }

            # å‘é€è¯·æ±‚
            response = requests.post(
                f"{self.base_url}/api/generate",
                json=data,
                timeout=self.timeout
            )
            response.raise_for_status()

            result = response.json()
            response_time = time.time() - start_time

            # åˆ›å»ºå“åº”å¯¹è±¡
            content = result.get("response", "").strip()

            # æ¸…ç†å“åº”å†…å®¹
            content = self._clean_response(content)

            ollama_response = OllamaResponse(
                content=content,
                model=self.model_name,
                tokens_used=result.get("eval_count", 0),
                response_time=response_time,
                metadata={
                    "total_duration": result.get("total_duration", 0),
                    "load_duration": result.get("load_duration", 0),
                    "eval_duration": result.get("eval_duration", 0),
                    "prompt_eval_count": result.get("prompt_eval_count", 0),
                    "eval_count": result.get("eval_count", 0),
                    "done": result.get("done", False)
                }
            )

            # æ›´æ–°ç»Ÿè®¡
            self.stats["total_tokens"] += ollama_response.tokens_used
            self.stats["total_time"] += response_time
            self.stats["successful_requests"] += 1

            # ç¼“å­˜å“åº”
            self.cache.set(prompt, self.model_name, ollama_response)

            self.logger.info(f"Ollamaå“åº”å®Œæˆ: è€—æ—¶{response_time:.2f}s, "
                             f"tokens={ollama_response.tokens_used}")

            return ollama_response

        except Exception as e:
            self.stats["failed_requests"] += 1
            self.logger.error(f"Ollamaç”Ÿæˆå¤±è´¥: {e}")

            # è¿”å›é”™è¯¯å“åº”
            return OllamaResponse(
                content=f"æŠ±æ­‰ï¼Œç”Ÿæˆå“åº”æ—¶å‡ºç°é”™è¯¯: {str(e)}",
                model=self.model_name,
                tokens_used=0,
                response_time=0.0,
                metadata={"error": str(e)}
            )

    def _clean_response(self, content: str) -> str:
        """æ¸…ç†å“åº”å†…å®¹"""
        # ç§»é™¤å¯èƒ½çš„åœæ­¢è¯æ®‹ç•™
        stop_words = ["Human:", "Assistant:", "User:", "AI:"]
        for stop_word in stop_words:
            if content.endswith(stop_word):
                content = content[:-len(stop_word)].strip()

        # ç§»é™¤å¤šä½™çš„æ¢è¡Œ
        content = content.replace("\n\n\n", "\n\n")

        return content.strip()

    def chat(self, messages: List[Dict[str, str]], **kwargs) -> OllamaResponse:
        """èŠå¤©æ¨¡å¼"""
        # å°†æ¶ˆæ¯åˆ—è¡¨è½¬æ¢ä¸ºå•ä¸ªprompt
        prompt_parts = []

        for msg in messages:
            role = msg.get("role", "user")
            content = msg.get("content", "")

            if role == "system":
                prompt_parts.append(f"System: {content}")
            elif role == "user":
                prompt_parts.append(f"Human: {content}")
            elif role == "assistant":
                prompt_parts.append(f"Assistant: {content}")

        prompt_parts.append("Assistant:")  # æç¤ºAIå¼€å§‹å›ç­”
        full_prompt = "\n\n".join(prompt_parts)

        return self.generate(full_prompt, **kwargs)

    def get_stats(self) -> Dict:
        """è·å–ä½¿ç”¨ç»Ÿè®¡"""
        total_requests = self.stats["requests"]
        cache_hit_rate = self.stats["cache_hits"] / max(total_requests, 1)
        success_rate = self.stats["successful_requests"] / max(total_requests, 1)

        actual_requests = total_requests - self.stats["cache_hits"]
        avg_response_time = self.stats["total_time"] / max(actual_requests, 1)

        return {
            "total_requests": total_requests,
            "cache_hit_rate": f"{cache_hit_rate:.1%}",
            "success_rate": f"{success_rate:.1%}",
            "total_tokens": self.stats["total_tokens"],
            "average_response_time": f"{avg_response_time:.2f}s",
            "current_model": self.model_name,
            "service_available": self.is_available(),
            "failed_requests": self.stats["failed_requests"]
        }

    def test_connection(self) -> Dict:
        """æµ‹è¯•è¿æ¥å’ŒåŸºæœ¬åŠŸèƒ½"""
        print("ğŸ” æµ‹è¯•Ollamaè¿æ¥...")

        # 1. æ£€æŸ¥Ollamaå®‰è£…
        if not self.is_ollama_installed():
            return {
                "status": "not_installed",
                "message": "Ollamaæœªæ£€æµ‹åˆ°",
                "suggestions": [
                    "è¯·ç¡®è®¤Ollamaå·²æ­£ç¡®å®‰è£…",
                    "Windows: æ£€æŸ¥ç¯å¢ƒå˜é‡PATH",
                    "é‡å¯å‘½ä»¤æç¤ºç¬¦åé‡è¯•"
                ]
            }

        # 2. æ£€æŸ¥æœåŠ¡å¯ç”¨æ€§
        if not self.is_available():
            return {
                "status": "service_unavailable",
                "message": "OllamaæœåŠ¡ä¸å¯ç”¨",
                "suggestions": [
                    "ç¡®è®¤OllamaæœåŠ¡æ­£åœ¨è¿è¡Œ: ollama serve",
                    "æ£€æŸ¥ç«¯å£æ˜¯å¦æ­£ç¡®: " + self.base_url,
                    "æ£€æŸ¥é˜²ç«å¢™è®¾ç½®"
                ]
            }

        # 3. æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
        available_models = self.list_models()
        print(f"å¯ç”¨æ¨¡å‹: {available_models}")

        if self.model_name not in available_models:
            # å°è¯•å¸¸è§çš„æ¨¡å‹åç§°å˜ä½“
            model_variants = [
                self.model_name,
                f"{self.model_name}:latest",
                self.model_name.split(":")[0] if ":" in self.model_name else f"{self.model_name}:latest"
            ]

            found_model = None
            for variant in model_variants:
                if variant in available_models:
                    found_model = variant
                    break

            if found_model:
                self.model_name = found_model
                print(f"âœ… æ‰¾åˆ°æ¨¡å‹å˜ä½“: {found_model}")
            else:
                return {
                    "status": "model_missing",
                    "message": f"æ¨¡å‹ {self.model_name} ä¸å­˜åœ¨",
                    "available_models": available_models,
                    "suggestions": [
                        f"ä½¿ç”¨å¯ç”¨æ¨¡å‹: {available_models[0] if available_models else 'æ— '}",
                        f"æˆ–æ‹‰å–æ‰€éœ€æ¨¡å‹: ollama pull {self.model_name}"
                    ]
                }

        # 4. æµ‹è¯•ç”ŸæˆåŠŸèƒ½
        try:
            test_prompt = "Hello! Please respond with 'Connection test successful.'"
            print(f"æµ‹è¯•æç¤º: {test_prompt}")

            response = self.generate(test_prompt, max_tokens=30, temperature=0.1)

            if "error" in response.metadata:
                return {
                    "status": "generation_failed",
                    "message": "ç”Ÿæˆæµ‹è¯•å¤±è´¥",
                    "error": response.metadata["error"]
                }

            return {
                "status": "success",
                "message": "è¿æ¥æµ‹è¯•æˆåŠŸ",
                "model": self.model_name,
                "response_time": f"{response.response_time:.2f}s",
                "test_response": response.content[:100],
                "tokens_used": response.tokens_used
            }

        except Exception as e:
            return {
                "status": "test_failed",
                "message": "è¿æ¥æµ‹è¯•å¤±è´¥",
                "error": str(e)
            }


# ä¼˜åŒ–çš„è®¾ç½®å‡½æ•°
def setup_ollama_auto():
    """è‡ªåŠ¨è®¾ç½®Ollamaï¼ˆé’ˆå¯¹ç°æœ‰ç¯å¢ƒä¼˜åŒ–ï¼‰"""
    print("ğŸš€ Ollamaè‡ªåŠ¨è®¾ç½®åŠ©æ‰‹")
    print("=" * 40)

    # åˆ›å»ºå®¢æˆ·ç«¯
    client = OllamaClient()

    # 1. æ£€æŸ¥Ollamaå®‰è£…
    if not client.is_ollama_installed():
        print("âŒ Ollamaæœªæ­£ç¡®æ£€æµ‹åˆ°")
        print("è¯·ç¡®è®¤:")
        print("  1. Ollamaå·²å®‰è£…")
        print("  2. ç¯å¢ƒå˜é‡PATHå·²é…ç½®")
        print("  3. é‡å¯äº†å‘½ä»¤æç¤ºç¬¦")
        return False, None

    print("âœ… Ollamaå·²å®‰è£…")

    # 2. æ£€æŸ¥æœåŠ¡çŠ¶æ€
    if not client.is_available():
        print("âŒ OllamaæœåŠ¡æœªè¿è¡Œ")
        print("è¯·åœ¨å¦ä¸€ä¸ªå‘½ä»¤æç¤ºç¬¦çª—å£è¿è¡Œ: ollama serve")
        print("ç„¶åä¿æŒè¯¥çª—å£æ‰“å¼€")
        return False, None

    print("âœ… OllamaæœåŠ¡è¿è¡Œä¸­")

    # 3. æ£€æŸ¥å¯ç”¨æ¨¡å‹
    available_models = client.list_models()
    print(f"ğŸ“¦ å½“å‰å·²å®‰è£… {len(available_models)} ä¸ªæ¨¡å‹")

    if available_models:
        print("å¯ç”¨æ¨¡å‹:")
        for model in available_models:
            print(f"  - {model}")

        # è‡ªåŠ¨é€‰æ‹©æœ€ä½³æ¨¡å‹
        preferred_models = ["llama3", "llama3:latest", "qwen2:7b", "qwen2:1.5b"]
        selected_model = None

        for preferred in preferred_models:
            if preferred in available_models:
                selected_model = preferred
                break

        if selected_model:
            client.model_name = selected_model
            print(f"âœ… è‡ªåŠ¨é€‰æ‹©æ¨¡å‹: {selected_model}")
        else:
            # ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨æ¨¡å‹
            client.model_name = available_models[0]
            print(f"âœ… ä½¿ç”¨æ¨¡å‹: {available_models[0]}")
    else:
        print("âš ï¸ æœªæ£€æµ‹åˆ°ä»»ä½•æ¨¡å‹")
        print("è¿™å¯èƒ½æ˜¯æ¨¡å‹åç§°æ ¼å¼é—®é¢˜ï¼Œå°è¯•ç»§ç»­...")

    # 4. æµ‹è¯•è¿æ¥
    test_result = client.test_connection()

    if test_result["status"] == "success":
        print(f"\nğŸ‰ è®¾ç½®å®Œæˆï¼")
        print(f"å½“å‰æ¨¡å‹: {client.model_name}")
        print(f"æµ‹è¯•å“åº”æ—¶é—´: {test_result['response_time']}")
        print(f"æµ‹è¯•å›ç­”: {test_result.get('test_response', 'N/A')}")
        return True, client
    else:
        print(f"\nâŒ è®¾ç½®å¤±è´¥: {test_result['message']}")
        if "suggestions" in test_result:
            print("å»ºè®®:")
            for suggestion in test_result["suggestions"]:
                print(f"  ğŸ’¡ {suggestion}")
        return False, None


def demo_ollama_client():
    """æ¼”ç¤ºOllamaå®¢æˆ·ç«¯åŠŸèƒ½"""
    print("ğŸ¤– Ollamaå®¢æˆ·ç«¯æ¼”ç¤º (llama3ä¼˜åŒ–ç‰ˆ)")
    print("=" * 50)

    # è‡ªåŠ¨è®¾ç½®
    success, client = setup_ollama_auto()
    if not success:
        print("\nâŒ è‡ªåŠ¨è®¾ç½®å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¸Šè¿°å»ºè®®åé‡è¯•")
        return

    # IoTå®‰å…¨ç›¸å…³æµ‹è¯•æŸ¥è¯¢
    test_queries = [
        {
            "query": "What is IoT security?",
            "category": "åŸºç¡€æ¦‚å¿µ"
        },
        {
            "query": "List the top 5 IoT security threats.",
            "category": "å¨èƒè¯†åˆ«"
        },
        {
            "query": "How to secure smart home devices?",
            "category": "é˜²æŠ¤å»ºè®®"
        },
        {
            "query": "Explain buffer overflow attacks on IoT devices.",
            "category": "æŠ€æœ¯åˆ†æ"
        }
    ]

    print(f"\nğŸ’¬ IoTå®‰å…¨æµ‹è¯•æŸ¥è¯¢:")
    for i, test_case in enumerate(test_queries, 1):
        query = test_case["query"]
        category = test_case["category"]

        print(f"\n{i}. ã€{category}ã€‘{query}")
        print("-" * 50)

        try:
            start_time = time.time()
            response = client.generate(query, max_tokens=200, temperature=0.7)
            total_time = time.time() - start_time

            print(f"âœ… ç”ŸæˆæˆåŠŸ")
            print(f"ğŸ“Š æ¨¡å‹: {response.model}")
            print(f"â±ï¸ æ€»è€—æ—¶: {total_time:.2f}s")
            print(f"ğŸ”§ ç”Ÿæˆè€—æ—¶: {response.response_time:.2f}s")
            print(f"ğŸ“„ Tokens: {response.tokens_used}")
            print(f"ğŸ“ å›ç­”:")
            print(f"{response.content}")

        except Exception as e:
            print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")

    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ“Š ä½¿ç”¨ç»Ÿè®¡:")
    stats = client.get_stats()
    for key, value in stats.items():
        print(f"  {key}: {value}")

    # ç¼“å­˜æµ‹è¯•
    print(f"\nğŸ”„ ç¼“å­˜åŠŸèƒ½æµ‹è¯•:")
    test_query = "What is IoT?"

    print("ç¬¬ä¸€æ¬¡æŸ¥è¯¢ï¼ˆæ— ç¼“å­˜ï¼‰:")
    start_time = time.time()
    response1 = client.generate(test_query, max_tokens=50)
    time1 = time.time() - start_time
    print(f"  è€—æ—¶: {time1:.2f}s")

    print("ç¬¬äºŒæ¬¡æŸ¥è¯¢ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰:")
    start_time = time.time()
    response2 = client.generate(test_query, max_tokens=50)
    time2 = time.time() - start_time
    print(f"  è€—æ—¶: {time2:.2f}s")
    print(f"  ç¼“å­˜åŠ é€Ÿ: {(time1 / time2):.1f}x" if time2 > 0 else "  ç¼“å­˜å‘½ä¸­")

    print(f"\nğŸ‰ æ¼”ç¤ºå®Œæˆï¼å®¢æˆ·ç«¯å·¥ä½œæ­£å¸¸ã€‚")


if __name__ == "__main__":
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

    # è¿è¡Œæ¼”ç¤º
    demo_ollama_client()